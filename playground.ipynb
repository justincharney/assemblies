{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437308eb",
   "metadata": {},
   "source": [
    "# Playground\n",
    "Figuring out how to work with the NEMO model.\n",
    "\n",
    "## Objectives\n",
    "1. Simulate each action (project, associate, merge) several times.\n",
    "2. For a given brain region $A$ (where we simulated activity) output a matrix $W \\in \\mathbb{R}^{n \\times t}$, where $n$ is number of neurons, $t$ is number of rounds, and $W_{ij}$ is the synaptic weight of neuron $i$ and time step $j$. We will cal this the **neural activity matrix** of $A$.\n",
    "    - Optional: Try out various combinations of brain regions and actions (ie., have multiple brain regions that project onto a single region where you extract weights from).\n",
    "    - Optional: Try the above for complex NEMO models, like sentence parsing or text generation.\n",
    "3. Run nonlinear (and maybe linear?) dimension reductions to achieve a neural manifold.\n",
    "    - Time Permits: compare this artifical neural manifold to a real neural manifold (though utility of comparison depends on tasks that these manifolds were dervied from).\n",
    "\n",
    "## NEMO Library\n",
    "The following files will be useful for our experiments:\n",
    "- `simulations.py`: contains code for running simulations of different NEMO actions.\n",
    "- `brain.py`: contains (all?) code for the NEMO model. We may need to modify this for our use case.\n",
    "- `brain_util.py`: contains code for saving/loading NEMO instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74def460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.brain as brain\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eab4633",
   "metadata": {},
   "source": [
    "# Understanding and Modifying NEMO Library\n",
    "Let us look at an example of simulating the project operation (which comes from `simulations.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afd602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_sim(n=1_000_000, k=1_000, p=0.01, beta=0.05, t=50):\n",
    "    \"\"\"Simulate repeated projections from a stimulus to a brain area.\n",
    "    \n",
    "    Args:\n",
    "        n: Number of neurons in an area A\n",
    "        k: Number of winners in A that fire at each round (instantaneous assembly size)\n",
    "        p: Connection probability\n",
    "        beta: Plasticity parameter\n",
    "        t: Number of projection rounds\n",
    "    \n",
    "    Returns:\n",
    "        List of support sizes over time: at each round, the number of distinct\n",
    "        neurons in A that have ever fired at least once.\n",
    "    \"\"\"\n",
    "    b = brain.Brain(p)\n",
    "    \n",
    "    # Create a stimulus (called \"stim\") modeled as k neurons that will fire\n",
    "    # together whenever \"stim\" is present.\n",
    "    b.add_stimulus(\"stim\", k)\n",
    "    \n",
    "    # Add a brain region (named \"A\") with n neurons. On each projection step,\n",
    "    # exactly k of these n neurons are chosen as winners (the active assembly\n",
    "    # in A at that time step).\n",
    "    b.add_area(\"A\", n, k, beta)\n",
    "        \n",
    "    # First step: let the stimulus \"stim\" project into A. The k most excited\n",
    "    # neurons in A fire, forming an initial assembly of size k tied to \"stim\".\n",
    "    b.project({\"stim\": [\"A\"]}, {})\n",
    "    \n",
    "    for i in range(t - 1):\n",
    "        # Subsequent steps: both the stimulus and the current winners in A\n",
    "        # project into A. This combines feedforward input from \"stim\" with\n",
    "        # recurrent input from A itself, further strengthening and growing\n",
    "        # the assembly over time.\n",
    "        b.project({\"stim\": [\"A\"]}, {\"A\": [\"A\"]})\n",
    "    \n",
    "    return b.area_by_name['A'].saved_w\n",
    "\n",
    "no_neurons_fired_time = project_sim()\n",
    "assert len(no_neurons_fired_time) == 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff608e0",
   "metadata": {},
   "source": [
    "Some things to note:\n",
    "- The number of neurons in region $A$ is denoted by `n`. In the above, `n = 1_000_000`. The number of rounds we are projecting for is `t = 50`. This produces a very large neural activity matrix $W \\in \\mathbb{R}^{1,000,000 \\times 50}$. **We should be careful and think about memory**.\n",
    "- Instead of the above, we could only consider the neurons associated with the assembly, denoted by `k`. In the above, `k = 1_000` wich means we would produce a neural activity matrix $W \\in \\mathbb{R}^{1,000 \\times 50}$.\n",
    "- The implementation above is a laxy implemenation (we never truly instantiate `n` neurons). To be able to keep track of the synaptic weights, we would need to use `b.add_explicit_area(...)`.\n",
    "\n",
    "Recall that we do two kind of projections: (i) stimulus into $A$; (ii) $A$ into $A$. There are two weights we can then extract:\n",
    "```py\n",
    "b.connectomes_by_stimulus[\"stim\"][\"A\"] # Length n array. Synaptic weights from stimulus into area A.\n",
    "b.connectomes[\"A\"][\"A\"]                # Matrix of size (n, n). Synaptic weight from one neuron in area A to another in area A.\n",
    "```\n",
    "\n",
    "Below is some code from GPT offering multiple solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca31b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_sim_with_weight_logging(\n",
    "    n                   = 10_000,\n",
    "    k                   = 1_000,\n",
    "    p                   = 0.01,\n",
    "    beta                = 0.05,\n",
    "    t                   = 50,\n",
    "    record_mode         = \"feedforward\", # \"feedforward\", \"recurrent\", or \"both\"\n",
    "    recurrent_aggregate = \"incoming\",    # \"incoming\", \"outgoing\", \"total\", or \"self\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Run T projection rounds and record synaptic weights for all n neurons in area A.\n",
    "\n",
    "    Args:\n",
    "        n: Number of neurons in an area A\n",
    "        k: Number of winners in A that fire at each round (instantaneous assembly size)\n",
    "        p: Connection probability\n",
    "        beta: Plasticity parameter\n",
    "        t: Number of projection rounds\n",
    "        \n",
    "        record_mode:\n",
    "            \"feedforward\": record only stim -> A weights.\n",
    "            \"recurrent\"  : record only A -> A recurrent weights (summarized per neuron).\n",
    "            \"both\"       : record both, in parallel.\n",
    "        \n",
    "        recurrent_aggregate:\n",
    "            How to reduce the n x n recurrent matrix to a length-n vector:\n",
    "                \"incoming\": sum over presynaptic neurons (column sums), which is total recurrent input into each neuron.\n",
    "                \"outgoing\": sum over postsynaptic neurons (row sums), which is total recurrent output from each neuron.\n",
    "                \"total\"   : incoming + outgoing.\n",
    "                \"self\"    : diagonal (self-connections only).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with:\n",
    "            result[\"saved_w\"]    : list of length t - support size (ever-fired neurons in A) after each round.\n",
    "            result[\"feedforward\"]: 2D array of shape (n, t) or None.\n",
    "                                   Assume you save the matrix as recurrent Then, feedforward[i, step] = weight\n",
    "                                   from \"stim\" -> neuron i in A after that step.\n",
    "            result[\"recurrent\"]  : 2D array of shape (n, t) or None.\n",
    "                                   Assume you save the matrix as recurrent. Then, recurrent[i, step] = aggregated\n",
    "                                   recurrent weight for neuron i in A after that step (definition controlled by\n",
    "                                   recurrent_aggregate).\n",
    "    \"\"\"\n",
    "    assert record_mode in (\"feedforward\", \"recurrent\", \"both\"), (\"Invalid record_mode\")\n",
    "    assert recurrent_aggregate in (\"incoming\", \"outgoing\", \"total\", \"self\"), (\"Invalid recurrent_aggregate\")\n",
    "\n",
    "    b = brain.Brain(p)\n",
    "    b.add_stimulus(\"stim\", k)\n",
    "\n",
    "    # Explicit area: all n neurons and their synapses are materialized.\n",
    "    b.add_explicit_area(\"A\", n, k, beta)\n",
    "\n",
    "    feedforward_history = []  # list of (n,) arrays\n",
    "    recurrent_history   = []  # list of (n,) arrays\n",
    "\n",
    "    def snapshot():\n",
    "        \"\"\"Record weights after a projection step.\"\"\"\n",
    "        if record_mode in (\"feedforward\", \"both\"):\n",
    "            # Vector of weights from stim -> each neuron in A (shape (n,))\n",
    "            w_ff = b.connectomes_by_stimulus[\"stim\"][\"A\"].copy()\n",
    "            feedforward_history.append(w_ff)\n",
    "\n",
    "        if record_mode in (\"recurrent\", \"both\"):\n",
    "            # Full recurrent matrix A -> A (shape (n, n))\n",
    "            W_rec = b.connectomes[\"A\"][\"A\"]\n",
    "            \n",
    "            # The recorded synaptic weights (initialze)\n",
    "            rec_vec = np.zeros(n)\n",
    "\n",
    "            if recurrent_aggregate == \"incoming\":\n",
    "                # Total recurrent input into each neuron (sum over presynaptic j)\n",
    "                # column i = sum_j W_rec[j, i]\n",
    "                rec_vec = W_rec.sum(axis=0)\n",
    "\n",
    "            elif recurrent_aggregate == \"outgoing\":\n",
    "                # Total recurrent output from each neuron (sum over postsynaptic i)\n",
    "                # row j = sum_i W_rec[j, i]\n",
    "                rec_vec = W_rec.sum(axis=1)\n",
    "\n",
    "            elif recurrent_aggregate == \"total\":\n",
    "                # Incoming + outgoing for each neuron\n",
    "                rec_in = W_rec.sum(axis=0)\n",
    "                rec_out = W_rec.sum(axis=1)\n",
    "                rec_vec = rec_in + rec_out\n",
    "\n",
    "            elif recurrent_aggregate == \"self\":\n",
    "                # Only self-connections W_rec[i, i]\n",
    "                rec_vec = np.diag(W_rec)\n",
    "\n",
    "            recurrent_history.append(rec_vec.copy())\n",
    "\n",
    "    # ============================================== PROJECTION STEPS ==============================================\n",
    "    # Step 1: only stim -> A (no recurrent yet)\n",
    "    b.project({\"stim\": [\"A\"]}, {})\n",
    "    snapshot()\n",
    "\n",
    "    # Steps 2, ..., t: stim -> A and A -> A (recurrent)\n",
    "    for _ in range(t - 1):\n",
    "        b.project({\"stim\": [\"A\"]}, {\"A\": [\"A\"]})\n",
    "        snapshot()\n",
    "\n",
    "    # =============================================== SAVING RESULTS ================================================\n",
    "    result = {\n",
    "        \"saved_w\": b.area_by_name[\"A\"].saved_w,\n",
    "        \"feedforward\": None,\n",
    "        \"recurrent\": None,\n",
    "    }\n",
    "\n",
    "    if feedforward_history:\n",
    "        # Shape (n, t): row = neruon index | col = time step\n",
    "        result[\"feedforward\"] = np.stack(feedforward_history, axis=0).T\n",
    "\n",
    "    if recurrent_history:\n",
    "        # Shape (n, t): row = neruon index | col = time step\n",
    "        result[\"recurrent\"] = np.stack(recurrent_history, axis=0).T\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bebd810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Only feedforward weights: W_ff[step, i] = stim -> A_i\n",
    "res_ff = project_sim_with_weight_logging(\n",
    "    n=2000, k=200, p=0.01, beta=0.05, t=20,\n",
    "    record_mode=\"feedforward\"\n",
    ")\n",
    "W_ff = res_ff[\"feedforward\"] # shape (2000, 20)\n",
    "\n",
    "# 2) Only recurrent incoming weights: W_rec[step, i] = sum_j W_rec[j,i]\n",
    "res_rec_in = project_sim_with_weight_logging(\n",
    "    n=2000, k=200, p=0.01, beta=0.05, t=20,\n",
    "    record_mode=\"recurrent\",\n",
    "    recurrent_aggregate=\"incoming\"\n",
    ")\n",
    "W_rec_in = res_rec_in[\"recurrent\"] # shape (2000, 20)\n",
    "\n",
    "# 3) Both feedforward and recurrent (e.g., outgoing)\n",
    "res_both = project_sim_with_weight_logging(\n",
    "    n=2000, k=200, p=0.01, beta=0.05, t=20,\n",
    "    record_mode=\"both\",\n",
    "    recurrent_aggregate=\"outgoing\"\n",
    ")\n",
    "W_ff      = res_both[\"feedforward\"] # shape (2000, 20)\n",
    "W_rec_out = res_both[\"recurrent\"]   # shape (2000, 20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
